{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from math import log, sqrt\n",
    "from itertools import combinations\n",
    "from __future__ import unicode_literals\n",
    "import classifiers\n",
    "import re\n",
    "import nltk\n",
    "from Politweet import get_tweets, get_transcript\n",
    "import ratings\n",
    "from sentiment import polarity_train, classify, prob_classify, plus_df, minus_df\n",
    "import pandas as pd\n",
    "from gensim import corpora, models, similarities\n",
    "from itertools import chain\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from operator import itemgetter\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "pd.set_option('display.max_colwidth', 1200)\n",
    "\n",
    "tweets = get_tweets(\"./datasets/tweets.tsv\")\n",
    "\n",
    "def cosine_distance(a, b):\n",
    "    cos = 0.0\n",
    "    a_tfidf = a[\"tfidf\"]\n",
    "    for token, tfidf in b[\"tfidf\"].iteritems():\n",
    "        if token in a_tfidf:\n",
    "            cos += tfidf * a_tfidf[token]\n",
    "    return cos\n",
    "\n",
    "def normalize(features):\n",
    "    norm = 1.0 / sqrt(sum(i**2 for i in features.itervalues()))\n",
    "    for k, v in features.iteritems():\n",
    "        features[k] = v * norm\n",
    "    return features\n",
    "\n",
    "def add_tfidf_to(documents):\n",
    "    tokens = {}\n",
    "    for id, doc in enumerate(documents):\n",
    "        tf = {}\n",
    "        doc[\"tfidf\"] = {}\n",
    "        doc_tokens = doc.get(\"tokens\", [])\n",
    "        for token in doc_tokens:\n",
    "            tf[token] = tf.get(token, 0) + 1\n",
    "        num_tokens = len(doc_tokens)\n",
    "        if num_tokens > 0:\n",
    "            for token, freq in tf.iteritems():\n",
    "                tokens.setdefault(token, []).append((id, float(freq) / num_tokens))\n",
    "\n",
    "    doc_count = float(len(documents))\n",
    "    for token, docs in tokens.iteritems():\n",
    "        idf = log(doc_count / len(docs))\n",
    "        for id, tf in docs:\n",
    "            tfidf = tf * idf\n",
    "            if tfidf > 0:\n",
    "                documents[id][\"tfidf\"][token] = tfidf\n",
    "\n",
    "    for doc in documents:\n",
    "        doc[\"tfidf\"] = normalize(doc[\"tfidf\"])\n",
    "\n",
    "def choose_cluster(node, cluster_lookup, edges):\n",
    "    new = cluster_lookup[node]\n",
    "    if node in edges:\n",
    "        seen, num_seen = {}, {}\n",
    "        for target, weight in edges.get(node, []):\n",
    "            seen[cluster_lookup[target]] = seen.get(\n",
    "                cluster_lookup[target], 0.0) + weight\n",
    "        for k, v in seen.iteritems():\n",
    "            num_seen.setdefault(v, []).append(k)\n",
    "        new = num_seen[max(num_seen)][0]\n",
    "    return new\n",
    "\n",
    "def majorclust(graph):\n",
    "    cluster_lookup = dict((node, i) for i, node in enumerate(graph.nodes))\n",
    "\n",
    "    count = 0\n",
    "    movements = set()\n",
    "    finished = False\n",
    "    while not finished:\n",
    "        finished = True\n",
    "        for node in graph.nodes:\n",
    "            new = choose_cluster(node, cluster_lookup, graph.edges)\n",
    "            move = (node, cluster_lookup[node], new)\n",
    "            if new != cluster_lookup[node] and move not in movements:\n",
    "                movements.add(move)\n",
    "                cluster_lookup[node] = new\n",
    "                finished = False\n",
    "\n",
    "    clusters = {}\n",
    "    for k, v in cluster_lookup.iteritems():\n",
    "        clusters.setdefault(v, []).append(k)\n",
    "\n",
    "    return clusters.values()\n",
    "\n",
    "def get_distance_graph(documents):\n",
    "    class Graph(object):\n",
    "        def __init__(self):\n",
    "            self.edges = {}\n",
    "\n",
    "        def add_edge(self, n1, n2, w):\n",
    "            self.edges.setdefault(n1, []).append((n2, w))\n",
    "            self.edges.setdefault(n2, []).append((n1, w))\n",
    "\n",
    "    graph = Graph()\n",
    "    doc_ids = range(len(documents))\n",
    "    graph.nodes = set(doc_ids)\n",
    "    for a, b in combinations(doc_ids, 2):\n",
    "        graph.add_edge(a, b, cosine_distance(documents[a], documents[b]))\n",
    "    return graph\n",
    "\n",
    "\n",
    "def main():\n",
    "    documents = [{\"tokens\": tweet[\"tokens\"]} for i, tweet in tweets.iterrows() if tweet[\"tokens\"]]\n",
    "    add_tfidf_to(documents)\n",
    "    dist_graph = get_distance_graph(documents)\n",
    "\n",
    "    for cluster in majorclust(dist_graph):\n",
    "        print \"=========\"\n",
    "        for doc_id in cluster:\n",
    "            print \" \".join(documents[doc_id][\"tokens\"])\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
