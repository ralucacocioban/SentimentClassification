{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "import classifiers\n",
    "import re\n",
    "import nltk\n",
    "from Politweet import get_tweets, get_transcript\n",
    "import ratings\n",
    "from sentiment import polarity_train, classify, prob_classify, plus_df, minus_df\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets = get_tweets(\"./datasets/tweets.tsv\")\n",
    "transcript = get_transcript(\"./datasets/transcript.csv\")\n",
    "todo = set(tweets.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets.to_pickle('./datasets/tweets.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "482 - polarity\n"
     ]
    }
   ],
   "source": [
    "polarity = polarity_train(tweets)\n",
    "polarity_minus = set(i for i,_ in classify(minus_df(tweets), polarity))\n",
    "polarity_plus = set(i for i,_ in classify(plus_df(tweets), polarity))\n",
    "polarities = tweets.reindex(polarity_minus | polarity_plus).sort()\n",
    "print len(polarities), \"- polarity\"\n",
    "todo -= set(polarities.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151 - marked as OTHERS\n"
     ]
    }
   ],
   "source": [
    "others = ratings.all(tweets, ratings.OTHER)\n",
    "print len(others), \"- marked as OTHERS\"\n",
    "todo -= set(others.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2605 - untagged\n"
     ]
    }
   ],
   "source": [
    "print len(todo), \"- untagged\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "left = tweets.reindex(todo).sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>tokens</th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tweet.id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>936466790</th>\n",
       "      <td> Jim Lehrer just directed the debate audience ... 30 seconds ... #tweetdebate</td>\n",
       "      <td> [jim, lehrer, direct, debate, audience, second, tweetdebate]</td>\n",
       "      <td> [{u'lemma': u'jim', u'token': u'jim', u'pos': u'NN'}, {u'lemma': u'lehrer', u'token': u'lehrer', u'pos': u'NN'}, {u'lemma': u'direct', u'token': u'directed', u'pos': u'VBN'}, {u'lemma': u'debate', u'token': u'debate', u'pos': u'NN'}, {u'lemma': u'audience', u'token': u'audience', u'pos': u'NN'}, {u'lemma': u'', u'token': u'', u'pos': u''}, {u'lemma': u'30', u'token': u'30', u'pos': u'CD'}, {u'lemma': u'second', u'token': u'seconds', u'pos': u'NNS'}, {u'lemma': u'', u'token': u'', u'pos': u''}, {u'lemma': u'', u'token': u'', u'pos': u''}, {u'lemma': u'tweetdebate', u'token': u'tweetdebate', u'pos': u'NN'}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>936466992</th>\n",
       "      <td>                                                     Here we go. #tweetdebate</td>\n",
       "      <td>                                                [tweetdebate]</td>\n",
       "      <td>                                                                                                                                                                                                                                                                                                                                                                                                                  [{u'lemma': u'go', u'token': u'go', u'pos': u'VBP'}, {u'lemma': u'', u'token': u'', u'pos': u''}, {u'lemma': u'', u'token': u'', u'pos': u''}, {u'lemma': u'tweetdebate', u'token': u'tweetdebate', u'pos': u'NN'}]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                content  \\\n",
       "tweet.id                                                                                  \n",
       "936466790  Jim Lehrer just directed the debate audience ... 30 seconds ... #tweetdebate   \n",
       "936466992                                                      Here we go. #tweetdebate   \n",
       "\n",
       "                                                                 tokens  \\\n",
       "tweet.id                                                                  \n",
       "936466790  [jim, lehrer, direct, debate, audience, second, tweetdebate]   \n",
       "936466992                                                 [tweetdebate]   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          clean  \n",
       "tweet.id                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
       "936466790  [{u'lemma': u'jim', u'token': u'jim', u'pos': u'NN'}, {u'lemma': u'lehrer', u'token': u'lehrer', u'pos': u'NN'}, {u'lemma': u'direct', u'token': u'directed', u'pos': u'VBN'}, {u'lemma': u'debate', u'token': u'debate', u'pos': u'NN'}, {u'lemma': u'audience', u'token': u'audience', u'pos': u'NN'}, {u'lemma': u'', u'token': u'', u'pos': u''}, {u'lemma': u'30', u'token': u'30', u'pos': u'CD'}, {u'lemma': u'second', u'token': u'seconds', u'pos': u'NNS'}, {u'lemma': u'', u'token': u'', u'pos': u''}, {u'lemma': u'', u'token': u'', u'pos': u''}, {u'lemma': u'tweetdebate', u'token': u'tweetdebate', u'pos': u'NN'}]  \n",
       "936466992                                                                                                                                                                                                                                                                                                                                                                                                                   [{u'lemma': u'go', u'token': u'go', u'pos': u'VBP'}, {u'lemma': u'', u'token': u'', u'pos': u''}, {u'lemma': u'', u'token': u'', u'pos': u''}, {u'lemma': u'tweetdebate', u'token': u'tweetdebate', u'pos': u'NN'}]  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "left[[\"content\",\"tokens\", \"clean\"]][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'watch', u'tweetdebate', u'drink', u'wait', u'start', u'cringing', u'mccain', u'blunder']\n",
      "[u'ahg3', u'michdot', u'yeah', u'slime', u'actually', u'second', u'choice', u'say', u'first', u'one', u'okay', u'roll']\n",
      "[u'prepare', u'heart', u'attack', u'tweetdebate']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[u'watch tweetdebate drink wait start cringing mccain blunder']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i, tweet in tweets[:3].iterrows():\n",
    "    print [token for token in tweet[\"tokens\"]]\n",
    "    \n",
    "tokens = [\" \".join(tweet[\"tokens\"]) for i,tweet in tweets.iterrows()]\n",
    "tokens[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "# count = Counter([token for i, tweet in tweets.iterrows() for token in tweet[\"tokens\"]])\n",
    "\n",
    "# vectorizer = TfidfVectorizer(lowercase=False)\n",
    "# tfidf_model = vectorizer.fit_transform(tokens)\n",
    "# feature_names = vectorizer.get_feature_names()\n",
    "# df = pd.DataFrame([\n",
    "#         (feature_names[col], tfidf_model[0, col])\n",
    "#         for col in tfidf_model.nonzero()[1]])\n",
    "# df.sort(1, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for col in tfidf_model.nonzero()[1]:\n",
    "#     print tfidf_model[0, col]\n",
    "#     print (feature_names[col], tfidf_model[0, col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tweets[tweets[\"content\"].str.contains(\"zoom\")][['content']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk import sent_tokenize, word_tokenize, FreqDist, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import classifiers\n",
    "\n",
    "neg = [\n",
    "    (t['clean'], 'neg')\n",
    "    for i,t in ratings.all(tweets, ratings.NEGATIVE).iterrows()]\n",
    "\n",
    "pos = [\n",
    "    (t['clean'], 'pos')\n",
    "    for i,t in ratings.all(tweets, ratings.POSITIVE).iterrows()]\n",
    "\n",
    "train, test = train_test_split(\n",
    "    pos + neg, \n",
    "    test_size = .2, \n",
    "    random_state = 10)\n",
    "\n",
    "def featurize(tweet):\n",
    "#     # tokenize into words\n",
    "#     tokens = [word for sent in sent_tokenize(tweet) for word in word_tokenize(sent)]\n",
    "\n",
    "#     # remove stopwords\n",
    "#     stop = stopwords.words('english')\n",
    "#     tokens = [token for token in tokens if token not in stop]\n",
    "\n",
    "#     # remove words less than three letters\n",
    "#     tokens = [word for word in tokens if len(word) >= 3]\n",
    "\n",
    "#     # lower capitalization\n",
    "#     tokens = [word.lower() for word in tokens]\n",
    "\n",
    "#     # lemmatize\n",
    "#     lmtzr = WordNetLemmatizer()\n",
    "#     tokens = [lmtzr.lemmatize(word) for word in tokens]\n",
    "    tokens = [token['lemma'] for token in tweet]\n",
    "    return tokens\n",
    "\n",
    "def run_classifier(train, test, vctrzr, clsfr):\n",
    "    # vectorize with above featurize() function and also vectorize test set with training fit\n",
    "    X_train = vctrzr.fit_transform([i[0] for i in train])\n",
    "    X_test = vctrzr.transform([i[0] for i in test])\n",
    "\n",
    "    # fit the classifier with training data\n",
    "    clsfr.fit(X_train, [i[1] for i in train])\n",
    "    \n",
    "    # grab accuracy\n",
    "    scr = clsfr.score(X_test, [i[1] for i in test])\n",
    "\n",
    "    # grab important features\n",
    "    imp_features = sorted(zip(clsfr.coef_[0], vctrzr.get_feature_names()))\n",
    "    \n",
    "    return scr, imp_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.78804347826086951,\n",
       " [(-8.3617082885758425, u'08'),\n",
       "  (-8.3617082885758425, u'100'),\n",
       "  (-8.3617082885758425, u'1000'),\n",
       "  (-8.3617082885758425, u'10000'),\n",
       "  (-8.3617082885758425, u'12'),\n",
       "  (-8.3617082885758425, u'1950s'),\n",
       "  (-8.3617082885758425, u'2004'),\n",
       "  (-8.3617082885758425, u'2019'),\n",
       "  (-8.3617082885758425, u'2030'),\n",
       "  (-8.3617082885758425, u'2050'),\n",
       "  (-8.3617082885758425, u'4'),\n",
       "  (-8.3617082885758425, u'42'),\n",
       "  (-8.3617082885758425, u'5000000'),\n",
       "  (-8.3617082885758425, u'700b'),\n",
       "  (-8.3617082885758425, u'72'),\n",
       "  (-8.3617082885758425, u'8'),\n",
       "  (-8.3617082885758425, u'9'),\n",
       "  (-8.3617082885758425, u'911'),\n",
       "  (-8.3617082885758425, u'915'),\n",
       "  (-8.3617082885758425, u'abroad'),\n",
       "  (-8.3617082885758425, u'abslutely'),\n",
       "  (-8.3617082885758425, u'accommodate'),\n",
       "  (-8.3617082885758425, u'accord'),\n",
       "  (-8.3617082885758425, u'accuse'),\n",
       "  (-8.3617082885758425, u'accusing'),\n",
       "  (-8.3617082885758425, u'act'),\n",
       "  (-8.3617082885758425, u'administration'),\n",
       "  (-8.3617082885758425, u'adult'),\n",
       "  (-8.3617082885758425, u'advisor'),\n",
       "  (-8.3617082885758425, u'afashionista'),\n",
       "  (-8.3617082885758425, u'affair'),\n",
       "  (-8.3617082885758425, u'afford'),\n",
       "  (-8.3617082885758425, u'afghani'),\n",
       "  (-8.3617082885758425, u'afraid'),\n",
       "  (-8.3617082885758425, u'age'),\n",
       "  (-8.3617082885758425, u'ago'),\n",
       "  (-8.3617082885758425, u'ah'),\n",
       "  (-8.3617082885758425, u'ahg3'),\n",
       "  (-8.3617082885758425, u'ahmadinejad'),\n",
       "  (-8.3617082885758425, u'ai')],\n",
       " [(-5.9638130157774718, u'nt'),\n",
       "  (-5.9638130157774718, u'time'),\n",
       "  (-5.9638130157774718, u'vote'),\n",
       "  (-5.8768016387878426, u'job'),\n",
       "  (-5.7967589311143062, u'go'),\n",
       "  (-5.7967589311143062, u'think'),\n",
       "  (-5.7226509589605836, u'spending'),\n",
       "  (-5.5891195663360609, u'point'),\n",
       "  (-5.5284949445196263, u'say'),\n",
       "  (-5.3171858508524199, u'win'),\n",
       "  (-4.9605109069136866, u'good'),\n",
       "  (-4.8652007271093627, u'3'),\n",
       "  (-4.8652007271093627, u'debate'),\n",
       "  (-4.6728288344619067, u'2'),\n",
       "  (-4.372724242011568, u'1'),\n",
       "  (-4.3363565978406928, u'debate08'),\n",
       "  (-4.1570156691848767, u'mccain'),\n",
       "  (-3.6982691944637756, u'current'),\n",
       "  (-3.2557628146752622, u'tweetdebate'),\n",
       "  (-3.196922314652328, u'obama')])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(tokenizer = featurize, binary = True, lowercase=False)\n",
    "classifier = MultinomialNB()\n",
    "\n",
    "score, imp_features = run_classifier(train, test, vectorizer, classifier)\n",
    "score, imp_features[0:40], imp_features[-21:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.80434782608695654,\n",
       " [(-1.4759941272554509, u'mccain'),\n",
       "  (-1.3825646012153336, u'nt'),\n",
       "  (-1.3338605212433292, u'stop'),\n",
       "  (-1.0532868309789634, u'false'),\n",
       "  (-1.044511571148947, u'yet'),\n",
       "  (-0.99757736410358078, u'bracelet'),\n",
       "  (-0.98357832813971879, u'miss'),\n",
       "  (-0.93125886610797315, u'different'),\n",
       "  (-0.91165056309537285, u'would'),\n",
       "  (-0.87604562897801563, u'mention'),\n",
       "  (-0.87475820281007299, u'much'),\n",
       "  (-0.87175753392485311, u'claim'),\n",
       "  (-0.82179803895363868, u'republican'),\n",
       "  (-0.80472382441829204, u'liar'),\n",
       "  (-0.78210207161398437, u'shit'),\n",
       "  (-0.76744079592402981, u'peacekeeper'),\n",
       "  (-0.7625471000411802, u'talk'),\n",
       "  (-0.75688883555742281, u'cheap'),\n",
       "  (-0.7452562632388392, u'lose'),\n",
       "  (-0.74291864696840226, u'hear')],\n",
       " [(0.91130887766322966, u'bring'),\n",
       "  (0.91606200882196787, u'father'),\n",
       "  (0.95427349168821629, u'debate'),\n",
       "  (0.95903771017843642, u'easily'),\n",
       "  (0.96394932814971468, u'clear'),\n",
       "  (0.98987629408193556, u'hold'),\n",
       "  (0.99780119083797147, u'blast'),\n",
       "  (1.0097638815234604, u'glad'),\n",
       "  (1.0711658444699239, u'great'),\n",
       "  (1.0737780991340335, u'nice'),\n",
       "  (1.1152008207274988, u'bringing'),\n",
       "  (1.1216115438145895, u'rock'),\n",
       "  (1.1560985991152755, u'tax'),\n",
       "  (1.2119724163723011, u'victory'),\n",
       "  (1.3518606481841957, u'win'),\n",
       "  (1.375599445372419, u'presidential'),\n",
       "  (1.432432744509571, u'love'),\n",
       "  (1.476755754711103, u'1'),\n",
       "  (1.4960586727638228, u'2'),\n",
       "  (1.6116576245631309, u'good')])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(tokenizer = featurize, lowercase=False)\n",
    "classifier = LinearSVC()\n",
    "\n",
    "score, imp_features = run_classifier(train, test, vectorizer, classifier)\n",
    "score, imp_features[0:20]\n",
    "score, imp_features[0:20], imp_features[-21:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
