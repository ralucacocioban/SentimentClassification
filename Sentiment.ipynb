{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "import classifiers\n",
    "import re\n",
    "import nltk\n",
    "from Politweet import get_tweets, get_transcript\n",
    "import ratings\n",
    "from sentiment import polarity_train, classify, prob_classify, plus_df, minus_df\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets = get_tweets(\"./datasets/tweets.tsv\")\n",
    "transcript = get_transcript(\"./datasets/transcript.csv\")\n",
    "todo = set(tweets.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets.to_pickle('./datasets/tweets.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "482 - polarity\n"
     ]
    }
   ],
   "source": [
    "polarity = polarity_train(tweets)\n",
    "polarity_minus = set(i for i,_ in classify(minus_df(tweets), polarity))\n",
    "polarity_plus = set(i for i,_ in classify(plus_df(tweets), polarity))\n",
    "polarities = tweets.reindex(polarity_minus | polarity_plus).sort()\n",
    "print len(polarities), \"- polarity\"\n",
    "todo -= set(polarities.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151 - marked as OTHERS\n"
     ]
    }
   ],
   "source": [
    "others = ratings.all(tweets, ratings.OTHER)\n",
    "print len(others), \"- marked as OTHERS\"\n",
    "todo -= set(others.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2605 - untagged\n"
     ]
    }
   ],
   "source": [
    "print len(todo), \"- untagged\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "left = tweets.reindex(todo).sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>tokens</th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tweet.id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>936466790</th>\n",
       "      <td> Jim Lehrer just directed the debate audience ... 30 seconds ... #tweetdebate</td>\n",
       "      <td> [jim, lehrer, direct, debate, audience, second, tweetdebate]</td>\n",
       "      <td> [{u'lemma': u'jim', u'token': u'jim', u'pos': u'NN'}, {u'lemma': u'lehrer', u'token': u'lehrer', u'pos': u'NN'}, {u'lemma': u'direct', u'token': u'directed', u'pos': u'VBN'}, {u'lemma': u'debate', u'token': u'debate', u'pos': u'NN'}, {u'lemma': u'audience', u'token': u'audience', u'pos': u'NN'}, {u'lemma': u'', u'token': u'', u'pos': u''}, {u'lemma': u'30', u'token': u'30', u'pos': u'CD'}, {u'lemma': u'second', u'token': u'seconds', u'pos': u'NNS'}, {u'lemma': u'', u'token': u'', u'pos': u''}, {u'lemma': u'', u'token': u'', u'pos': u''}, {u'lemma': u'tweetdebate', u'token': u'tweetdebate', u'pos': u'NN'}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>936466992</th>\n",
       "      <td>                                                     Here we go. #tweetdebate</td>\n",
       "      <td>                                                [tweetdebate]</td>\n",
       "      <td>                                                                                                                                                                                                                                                                                                                                                                                                                  [{u'lemma': u'go', u'token': u'go', u'pos': u'VBP'}, {u'lemma': u'', u'token': u'', u'pos': u''}, {u'lemma': u'', u'token': u'', u'pos': u''}, {u'lemma': u'tweetdebate', u'token': u'tweetdebate', u'pos': u'NN'}]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                content  \\\n",
       "tweet.id                                                                                  \n",
       "936466790  Jim Lehrer just directed the debate audience ... 30 seconds ... #tweetdebate   \n",
       "936466992                                                      Here we go. #tweetdebate   \n",
       "\n",
       "                                                                 tokens  \\\n",
       "tweet.id                                                                  \n",
       "936466790  [jim, lehrer, direct, debate, audience, second, tweetdebate]   \n",
       "936466992                                                 [tweetdebate]   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          clean  \n",
       "tweet.id                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
       "936466790  [{u'lemma': u'jim', u'token': u'jim', u'pos': u'NN'}, {u'lemma': u'lehrer', u'token': u'lehrer', u'pos': u'NN'}, {u'lemma': u'direct', u'token': u'directed', u'pos': u'VBN'}, {u'lemma': u'debate', u'token': u'debate', u'pos': u'NN'}, {u'lemma': u'audience', u'token': u'audience', u'pos': u'NN'}, {u'lemma': u'', u'token': u'', u'pos': u''}, {u'lemma': u'30', u'token': u'30', u'pos': u'CD'}, {u'lemma': u'second', u'token': u'seconds', u'pos': u'NNS'}, {u'lemma': u'', u'token': u'', u'pos': u''}, {u'lemma': u'', u'token': u'', u'pos': u''}, {u'lemma': u'tweetdebate', u'token': u'tweetdebate', u'pos': u'NN'}]  \n",
       "936466992                                                                                                                                                                                                                                                                                                                                                                                                                   [{u'lemma': u'go', u'token': u'go', u'pos': u'VBP'}, {u'lemma': u'', u'token': u'', u'pos': u''}, {u'lemma': u'', u'token': u'', u'pos': u''}, {u'lemma': u'tweetdebate', u'token': u'tweetdebate', u'pos': u'NN'}]  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "left[[\"content\",\"tokens\", \"clean\"]][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'watch', u'tweetdebate', u'drink', u'wait', u'start', u'cringing', u'mccain', u'blunder']\n",
      "[u'ahg3', u'michdot', u'yeah', u'slime', u'actually', u'second', u'choice', u'say', u'first', u'one', u'okay', u'roll']\n",
      "[u'prepare', u'heart', u'attack', u'tweetdebate']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[u'watch tweetdebate drink wait start cringing mccain blunder']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i, tweet in tweets[:3].iterrows():\n",
    "    print [token for token in tweet[\"tokens\"]]\n",
    "    \n",
    "tokens = [\" \".join(tweet[\"tokens\"]) for i,tweet in tweets.iterrows()]\n",
    "tokens[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "# count = Counter([token for i, tweet in tweets.iterrows() for token in tweet[\"tokens\"]])\n",
    "\n",
    "# vectorizer = TfidfVectorizer(lowercase=False)\n",
    "# tfidf_model = vectorizer.fit_transform(tokens)\n",
    "# feature_names = vectorizer.get_feature_names()\n",
    "# df = pd.DataFrame([\n",
    "#         (feature_names[col], tfidf_model[0, col])\n",
    "#         for col in tfidf_model.nonzero()[1]])\n",
    "# df.sort(1, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for col in tfidf_model.nonzero()[1]:\n",
    "#     print tfidf_model[0, col]\n",
    "#     print (feature_names[col], tfidf_model[0, col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tweets[tweets[\"content\"].str.contains(\"zoom\")][['content']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk import sent_tokenize, word_tokenize, FreqDist, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import classifiers\n",
    "\n",
    "neg = [\n",
    "    (t['content'], 'neg')\n",
    "    for i,t in ratings.all(tweets, ratings.NEGATIVE).iterrows()]\n",
    "\n",
    "pos = [\n",
    "    (t['content'], 'pos')\n",
    "    for i,t in ratings.all(tweets, ratings.POSITIVE).iterrows()]\n",
    "\n",
    "train, test = train_test_split(\n",
    "    pos + neg, \n",
    "    test_size = .2, \n",
    "    random_state = 10)\n",
    "\n",
    "\n",
    "def run_classifier(train, test, vctrzr, clsfr):\n",
    "    # vectorize with above featurize() function and also vectorize test set with training fit\n",
    "    X_train = vctrzr.fit_transform([i[0] for i in train])\n",
    "    X_test = vctrzr.transform([i[0] for i in test])\n",
    "\n",
    "    # fit the classifier with training data\n",
    "    clsfr.fit(X_train, [i[1] for i in train])\n",
    "    \n",
    "    # grab accuracy\n",
    "    scr = clsfr.score(X_test, [i[1] for i in test])\n",
    "\n",
    "    # grab important features\n",
    "    imp_features = sorted(zip(clsfr.coef_[0], vctrzr.get_feature_names()))\n",
    "    \n",
    "    return scr, imp_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.79347826086956519,\n",
       " [(-8.2817239904113915, u\"'08\"),\n",
       "  (-8.2817239904113915, u\"'no\"),\n",
       "  (-8.2817239904113915, u\"'were\"),\n",
       "  (-8.2817239904113915, u'*bored*'),\n",
       "  (-8.2817239904113915, u'*gasp*'),\n",
       "  (-8.2817239904113915, u'*holy'),\n",
       "  (-8.2817239904113915, u'+hesitation'),\n",
       "  (-8.2817239904113915, u'-1pt'),\n",
       "  (-8.2817239904113915, u'..http'),\n",
       "  (-8.2817239904113915, u'..who'),\n",
       "  (-8.2817239904113915, u'.but'),\n",
       "  (-8.2817239904113915, u'.tired'),\n",
       "  (-8.2817239904113915, u'.to'),\n",
       "  (-8.2817239904113915, u'//bit.ly/2zhw59'),\n",
       "  (-8.2817239904113915, u'//janosschumacher.wordpress.com/'),\n",
       "  (-8.2817239904113915, u'//poprl.com/1ln'),\n",
       "  (-8.2817239904113915, u'//snipr.com/3vlts'),\n",
       "  (-8.2817239904113915, u'//tinyurl.com/43v3kc'),\n",
       "  (-8.2817239904113915, u'//tinyurl.com/4dolsf'),\n",
       "  (-8.2817239904113915, u'//tinyurl.com/5pabty')],\n",
       " [(-5.9791388974173456, u'need'),\n",
       "  (-5.9791388974173456, u'think'),\n",
       "  (-5.8838287176130208, u'issue'),\n",
       "  (-5.8838287176130208, u\"n't\"),\n",
       "  (-5.8838287176130208, u'nice'),\n",
       "  (-5.8838287176130208, u'time'),\n",
       "  (-5.8838287176130208, u'win'),\n",
       "  (-5.7968173406233916, u'job'),\n",
       "  (-5.7167746329498552, u'bringing'),\n",
       "  (-5.7167746329498552, u'saying'),\n",
       "  (-5.6426666607961327, u'point'),\n",
       "  (-5.573673789309181, u'...'),\n",
       "  (-5.573673789309181, u'spending'),\n",
       "  (-5.573673789309181, u'won'),\n",
       "  (-4.9858871244070624, u'good'),\n",
       "  (-4.7852164289449117, u'debate'),\n",
       "  (-4.2563722996762419, u'debate08'),\n",
       "  (-4.0920692483849663, u'mccain'),\n",
       "  (-3.6182848962993246, u'current'),\n",
       "  (-3.1757785165108112, u'tweetdebate')])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "classifier = MultinomialNB()\n",
    "\n",
    "score, imp_features = run_classifier(train, test, vectorizer, classifier)\n",
    "score, imp_features[0:20], imp_features[-21:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.80434782608695654,\n",
       " [(-1.3662136589407419, u\"n't\"),\n",
       "  (-1.2755517204200266, u'stop'),\n",
       "  (-1.1691237214435368, u'yet'),\n",
       "  (-1.0126262953426366, u'false'),\n",
       "  (-0.90765961106097681, u'scared'),\n",
       "  (-0.89515359882356027, u'glazing'),\n",
       "  (-0.88639306426213349, u'would'),\n",
       "  (-0.87663353280317768, u'look'),\n",
       "  (-0.85991625116971737, u'shit'),\n",
       "  (-0.85385154011999642, u'mccain'),\n",
       "  (-0.84724695060498179, u'much'),\n",
       "  (-0.83362521486567753, u'liar'),\n",
       "  (-0.82612768739431086, u'bracelet'),\n",
       "  (-0.81656323968661337, u'different'),\n",
       "  (-0.80828146641973819, u'mention'),\n",
       "  (-0.78159715475092795, u'economic'),\n",
       "  (-0.77924594852879059, u'talking'),\n",
       "  (-0.77757311211144553, u'republican'),\n",
       "  (-0.76487998275696789, u'threat'),\n",
       "  (-0.75401452247416489, u'got')],\n",
       " [(0.85810726828730854, u'debate'),\n",
       "  (0.87204986809084051, u'understanding'),\n",
       "  (0.90998464241723553, u'twitter'),\n",
       "  (0.92142895089614829, u'festooned'),\n",
       "  (0.92734672179638789, u'win'),\n",
       "  (0.96605844647135675, u'clear'),\n",
       "  (0.97330035409405002, u'admitting'),\n",
       "  (0.98390283736775208, u'blast'),\n",
       "  (1.0268395690438032, u'great'),\n",
       "  (1.0393663681696033, u'interesting'),\n",
       "  (1.0747616305687644, u'love'),\n",
       "  (1.1192571185717999, u'victory'),\n",
       "  (1.129836998373172, u'rock'),\n",
       "  (1.1368282108514014, u'tax'),\n",
       "  (1.1673020373609522, u'presidential'),\n",
       "  (1.2910907859897285, u'nice'),\n",
       "  (1.30093841537786, u'won'),\n",
       "  (1.398425379523466, u'tweetdebate'),\n",
       "  (1.4968272459250906, u'bringing'),\n",
       "  (1.7570836264908545, u'good')])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(tokenizer = featurize)\n",
    "classifier = LinearSVC()\n",
    "\n",
    "score, imp_features = run_classifier(train, test, vectorizer, classifier)\n",
    "score, imp_features[0:20]\n",
    "score, imp_features[0:20], imp_features[-21:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
